{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b0c93d",
   "metadata": {},
   "source": [
    "# Curso 1: Neural Networks and Deep Learning\n",
    "## RafaCastle\n",
    "\n",
    "## Semana 3\n",
    "## Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fba880",
   "metadata": {},
   "source": [
    "De manera poco formal, las redes neuronales pueden representarse como un conglomerado de regresiones lineales. En la siguiente figura se muestran 3 diagramas donde los primeros 2 corresponden al de una regresión lineal, donde $\\overline{y} = a$. El tercero muestra una red neuronal donde los 3 se aglomeran. \n",
    "\n",
    "![Title](Imágenes/1.3.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2bac8a",
   "metadata": {},
   "source": [
    "Cabe destacar que para este caso, los parámetros $\\omega$ y $b$ están relacionados para cada capa (layer) en la red neuronal. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ce6ae",
   "metadata": {},
   "source": [
    "## Representación de una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4d1a2",
   "metadata": {},
   "source": [
    "Las redes neuronales se dividen en capas, en orden de izquierda a derecha las capas son:\n",
    "\n",
    " - Input layer: Capa de inputs\n",
    " - Hidden layers: Capas ocultas\n",
    " - Output layer: Capa de outputs\n",
    " \n",
    "Se utiliza la notación para diferenciar las capas como $a^{[0]} = X$ para la capa de inputs, $a^{[1]}, a^{[2]}, ... a^{[n-1]}$ para las capas ocultas en una red neuronal de $n+1$ capas (contando la capa de inputs) y $a^{[n]} = \\overline{y}$ para la capa de outputs. \n",
    "\n",
    "![Title](Imágenes/1.3.2.png)\n",
    "\n",
    "Generalmente la capa de inputs no se cuenta, por lo que la red neuronal anterior sería de 2 capas. Los parámetros asociados $\\omega$ y $b$ para está red neuronal se dividen en capas. Para la primera capa (la capa oculta), el parámetro $\\omega^{[1]}$ es de dimensión $4 \\times 3$, 4 por que la capa tiene 4 neruonas y 3 porque la capa de inputs tiene 3. El parámetro $b^{[1]}$ tiene dimensión $4 \\times 1$ por que hay 4 neuronas. En el caso de la segunda capa, el parámetro $w^{[2]}$ sería de dimensión $1 \\times 4$ y el parámetro $b^{[2]}$ de $1 \\times 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de3ddd",
   "metadata": {},
   "source": [
    "## Calculando el output de una red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e446c",
   "metadata": {},
   "source": [
    "A continuación se muestra la representación de una red neuronal, es notorio que los superíndices de los parámetros denotan la capa a la cual pertenece la neurona, mientras que los subíndices indican qué neurona es.\n",
    "\n",
    "![Title](Imágenes/1.3.3.png)\n",
    "\n",
    "De esta manera, tenemos el siguiente sistema de ecuaciones:\n",
    "\n",
    "![Title](Imágenes/1.3.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279e039",
   "metadata": {},
   "source": [
    "De manera matricial las ecuaciones puede escribirse como:\n",
    "\n",
    "$$\n",
    "w^{[1]T} x + b^{[1]} = z^{[1]}\n",
    "$$\n",
    "\n",
    "con $w^{[1]T}$ una matriz de $4 \\times 3$, $x^{[1]}$ de dimensión $3 \\times 1$ y $b^{[1]}$ y $z^{[1]}$ de $4 \\times 1$. Así, la matriz $a^{[1]}$ es también de dimensión $4 \\times 1$ y puede representarse como:\n",
    "\n",
    "$$\n",
    "a^{[1]} = \\sigma (z^{[1]})\n",
    "$$\n",
    "\n",
    "El vector $x$ puede representarse como $x = a^{[0]}$. Ahora, para el caso de la segunda capa se tiene el siguiente sistema:\n",
    "\n",
    "![Title](Imágenes/1.3.5.png)\n",
    "\n",
    "Cabe destacar que la entrada i-ésima de uno de estos vectores, digamos $a^{[1]}$ está denotada por $a^{[1](i)}$. Es importante no manejar las entradas de los vectores en loops for, sino tratarlos como matrices ayudándose de la librería Numpy para optimizar los cálculos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1a647",
   "metadata": {},
   "source": [
    "## Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab64642",
   "metadata": {},
   "source": [
    "Hasta ahora hemos estado usando la función del sigmoide para activar las capas de las redes neuronales, pero en algunos casos se pueden utilizar funciones que arrojan mejores resultados. Es decir,  el valor $a^{[i]}$ puede ser calculado por una función $g(z^{[i]})$, que bien puede ser $g(z) = tanh(z)$. \n",
    "\n",
    "![Title](Imágenes/1.3.6.png)\n",
    "\n",
    "Esta función es en general superior a la función del sigmoide exceptuando los casos de clasificación binaria, donde el valor final debe ser estrictamente 0 o 1, en ese caso se debe usar la función del sigmoide en la última capa, de hecho es recomendable solo usar la función del sigmoide en los problemas de clasificación.\n",
    "\n",
    "Una desventaja de la función $tanh(z)$ y de la función del sigmoide esque cuando el argumento toma valores muy grandes o muy pequeños, la derivada de la función se aproxima demasiado a 0, por lo que alenta el algoritmo de gradient descent. Por lo que se ocupan las funciones ReLu para esos casos (véase C1W1). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412935e6",
   "metadata": {},
   "source": [
    "## ¿Porqué se necesitan las funciones de activación no lineales?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfcc105",
   "metadata": {},
   "source": [
    "Tomemos una red neuronal de 2 capas de manera analítica:\n",
    "\n",
    "$$\n",
    "z^{[1]} = W^{[1]}x + b^{[1]} \\hspace{1cm} a^{[1]} = g^{[1]}(z^{[1]})\n",
    "$$\n",
    "\n",
    "$$\n",
    "z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\hspace{1cm} a^{[2]} = g^{[2]}(z^{[2]})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068c20d",
   "metadata": {},
   "source": [
    "Supongamos que la función $g$ es lineal, o sea $g(z) = z_o z + z_1$. En ese caso tendríamos:\n",
    "\n",
    "$$\n",
    "a^{[1]} = g(z^{[1]}) = z_o (W^{[1]} x + b^{[1]}) +z_1 =  (z_o W^{[1]}) x + (z_o b^{[1]} + z_1)\n",
    "$$\n",
    "\n",
    "Si redefinimos $(z_o W^{[1]})  = a_0$ y $(z_o b^{[1]} + z_1) = a_1$ tenemos:\n",
    "\n",
    "$$\n",
    "a^{[1]} = a_o x + a_1\n",
    "$$\n",
    "\n",
    "Luego, para obtener la siguiente capa tendríamos:\n",
    "\n",
    "$$\n",
    "z^{[2]} =  W^{[2]} a^{[1]} + b^{[2]} \n",
    "$$\n",
    "\n",
    "que de manera análoga nos regresa una ecuación lineal. Por lo que una red neuronal con una capa lineal no resulta más útil que una regresión lineal simple. Lo mismo pasa si se utiliza una función sigmoide en la útila capa, la siguiente red neuronal no sería más útil que una regresión logística (sin ninguna capa oculta):\n",
    "\n",
    "![Title](Imágenes/1.3.7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98105773",
   "metadata": {},
   "source": [
    "En general las funciones de activación lineales se utilizan en la última capa para problemas de regresión, pero las capas ocultas en general deben tener funciones no lineales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835e5ee",
   "metadata": {},
   "source": [
    "## Gradient descent para redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50210522",
   "metadata": {},
   "source": [
    "Resumiendo, si tenemos una red neuronal con $c$ capas y con $n^{[j]}$ neruonas en la capa $j$. Entonces la dimensión de los parámetros está dada por $dim(w^{[j]}) = (n^{[j]}, n^{[j-1]})$ y $dim(b^{[j]}) = (n^{[j]}, 1)$. Donde la función de pérdida está dada por \n",
    "\n",
    "$$\n",
    "J(w^{[1]}, b^{[1]},..., w^{[c]}, b^{[c]}) = \\frac{1}{m} \\sum_{i=1}^m L(\\overline{y},y)\n",
    "$$\n",
    "\n",
    "con $\\overline{y} = a^{[c]}$ y el algoritmo de gradient descent entrena a los parámetros de modo que se repite:\n",
    "\n",
    "$$\n",
    "w^{[i]} = w^{[i]} - \\alpha \\frac{\\partial J}{\\partial w^{[i]}} \\hspace{1cm} b^{[i]} = b^{[i]} - \\alpha \\frac{\\partial J}{\\partial b^{[i]}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3af7f4",
   "metadata": {},
   "source": [
    "Hasta que los parámetros converjan a algún valor. Así las propagaciones forward y backward siguen las siguientes lógicas:\n",
    "\n",
    "Forward:\n",
    "![Title](Imágenes/1.3.81.png)\n",
    "\n",
    "Backward:\n",
    "![Title](Imágenes/1.3.9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8140ba",
   "metadata": {},
   "source": [
    "## Inicialización aleatoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f487e",
   "metadata": {},
   "source": [
    "No es posible inicializar todos los parámetros igual a 0 en una red neuronal, por lo que es requerido inicializarlos de manera aleatoria para iniciar con el algoritmo gradient descent. Ilustremos porque no es posible que los parámetros inicien como 0. Supongamos que se tiene la siguiente red neuronal:\n",
    "\n",
    "![Title](Imágenes/1.3.10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b9eaf",
   "metadata": {},
   "source": [
    "En este caso se tendría que $a^{[1]}_1 = a^{[1]}_2$ y $dz^{[1]}_1 = dz^{[1]}_2$ . Por lo que las neuronas $a^{[1]}_1$ y $a^{[1]}_2$ estarían haciendo lo mismo desde el inicio y durante cada iteración. Por lo tanto no tendría sentido tener mas de una neurona en una capa oculta de la red neuronal. \n",
    "\n",
    "La solución a este problema es iniciar a los parámetros $W$ de manera aleatoria, los parámetros $b$ si pueden iniciar siendo 0 sin generar este problema. Por lo general se les da un valor pequeño a los valores $W$ para evitar caer en un lugar dentro del dominio de la función de activación donde ésta tome valores casi constantes:\n",
    "\n",
    "![Title](Imágenes/1.3.11.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
