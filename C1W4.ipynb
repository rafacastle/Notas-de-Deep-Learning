{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "564cec66",
   "metadata": {},
   "source": [
    "# Curso 1: Neural Networks and Deep Learning\n",
    "## RafaCastle\n",
    "\n",
    "## Semana 4\n",
    "## Redes neuronales profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c09a8a",
   "metadata": {},
   "source": [
    "Las redes neuronales profundas (con mayor cantidad de capas) generalmente logran aprenden modelos que las redes neuronales más superficiales no. \n",
    "\n",
    "![Title](Imágenes/1.4.1.png)\n",
    "\n",
    "Notación:\n",
    "\n",
    " - $L$ es el número de capas que tiene una red neuronal (sin contar la capa de inputs)\n",
    " - $n^{[l]}$ es el número de neuronas que tiene la capa $l$ ($l=0$ para la capa de inputs)\n",
    " - $n^{[0]} = n_x$ es el número de neuronas en la capa de inputs\n",
    " - $a^{[l]}$ función de activación en $l$\n",
    " - $w^{[l]}$ y $b^{[l]}$ parámetros de la capa $l$\n",
    " - $\\overline{y} = a^{[L]}$ es el output final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4683535",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110aaa0",
   "metadata": {},
   "source": [
    "Para una red neuronal en general se tiene la propagación:\n",
    "\n",
    "$$\n",
    "z^{[i]} = w^{[i]} a^{[i-1]} + b^{[i]} \\hspace{1cm} a^{[i]} = g^{[i]} (z^{[i]})\n",
    "$$\n",
    "\n",
    "Donde $a^{[0]}$ está dada por los inputs. Esta notación da pie a usar for loops, en general se trata de evitar este tipo de loops en el mundo del Machine Learning, sin embargo en este caso resultan de vital importancia para implementar la propagación forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ad9b1",
   "metadata": {},
   "source": [
    "## Intuición sobre las redes profundas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc373ebd",
   "metadata": {},
   "source": [
    "¿Por qué una red neuronal debe ser profunda para aprender funciones complejas? Ilustremos la respuesta con un ejemplo:\n",
    "\n",
    "![Title](Imágenes/1.4.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e546de",
   "metadata": {},
   "source": [
    "Supongamos que se busca armar una red neuronal que aprenda a reconocer distintos rostros, como se muestra en la imágen, la primera capa de una red convolucional (que se verá en próximos notebooks de este curso) identificaría las líneas de la imagen, en la segunda capa se reconocerían distintos rasgos de un rostro formados por las líneas de la capa anterior, como ojos, boca, nariz, etc. En la tercera capa podría empezar a reconocer realmente los distintos rostros formados por los rasgos de la capa anterior y en la cuarta arroja un resultado (si hubo match o no, por ejemplo). \n",
    "\n",
    "Así, una red neuronal va entendiendo funciones simples en las primeras capas y funciones más complejas en las últimas. \n",
    "\n",
    "## Construyendo los bloques de una red neuronal profunda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199fab45",
   "metadata": {},
   "source": [
    "En la siguiente figura se ilustra el funcionamiento de las propagaciones forward y backward:\n",
    "\n",
    "![Title](Imágenes/1.4.3.png)\n",
    "\n",
    "Si hacemos un resumen, tendríamos lo siguiente:\n",
    "\n",
    "### Forward Propagation para la capa $l$\n",
    "\n",
    " - Inputs: $a^{[l-1]}$\n",
    " - Outputs: $a^{[l]}$\n",
    " - Cache: $z^{[l]}, w^{[l]}, b^{[l]}$ \n",
    " \n",
    "El cache es un valor que se guarda para ejecutar la propagación backward en esa misma capa.\n",
    "\n",
    "![Title](Imágenes/1.4.4.png)\n",
    " \n",
    "### Backward Propagation para la capa $l$\n",
    "\n",
    " - Inputs: $da^{[l]}$\n",
    " - Outputs: $da^{[l-1]}, dw^{[l]}, db^{[l]}$\n",
    " \n",
    "![Title](Imágenes/1.4.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab51222",
   "metadata": {},
   "source": [
    "Fue mostrado en un notebook anterior que \n",
    "\n",
    "$$\n",
    "da^{[L]} = - \\frac{y}{a} + \\frac{1-y}{1-a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b845d7",
   "metadata": {},
   "source": [
    "## Parámetros vs hyperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8de6b",
   "metadata": {},
   "source": [
    "Existen parámetros que son relevantes para el output de una red neuronal pero que no varían con las iteraciones de aprendizaje que ésta requiere para aprender. Estos son denominados hyperparámetros y controlan a los parámetros $w$ y $b$. \n",
    "\n",
    "Los hyperparámetros que hemos visto hasta ahora son:\n",
    "\n",
    " 1. El número de iteraciones\n",
    " 2. La cantidad de capas ocultas\n",
    " 3. La cantidad de neuronas en las capas\n",
    " 4. La función de activación en las capas\n",
    " 5. El coeficiente de aprendizaje $\\alpha$\n",
    " \n",
    "En general el coeficiente de aprendizaje es elegible cuando minimice a la función de pérdida $J$ en la menro cantidad de iteraciones posibles, lógicas similares aplican para los otros hyperparámetros cuidando siempre no caer en el overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
